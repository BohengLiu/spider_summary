{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "\n",
    "# 爬虫的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 什么是爬虫？\n",
    "\n",
    "网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。\n",
    "\n",
    "## 爬虫的工作流程\n",
    "\n",
    "网络爬虫基本的工作流程是从一个根URL开始，抓取页面，解析页面中所有的URL，将还没有抓取过的URL放入工作队列中，之后继续抓取工作队列中的URL，重复抓取、解析，将解析到的url放入工作队列的步骤，直到工作队列为空为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 网页爬取\n",
    "## 常用的库\n",
    "### requests,BeautifulSoup4,scrapy,selenium,正则表达式\n",
    "## 常用工具软件\n",
    "### 浏览器，抓包软件，webdriver，无头浏览器如PhantomJS\n",
    "## 网页爬取的伪装介绍\n",
    "## 构造header\n",
    "### headers池\n",
    "## 获得cookie\n",
    "## post传输信息\n",
    "##  selenium\n",
    "## ip池\n",
    "## 模拟登陆\n",
    "## post传输信息模拟登陆\n",
    "## selenium模拟登陆\n",
    "\n",
    "# 内容提取\n",
    "\n",
    "## 正则表达式re\n",
    "\n",
    "## BeautifulSoup4\n",
    "\n",
    "## lxml\n",
    "\n",
    "##  各种方式的比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 数据存储\n",
    "## MongoDB\n",
    "## Mysql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 并发\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "我们希望通过并发执行来加快爬虫抓取页面的速度。一般的实现方式有三种：\n",
    "\n",
    "> * 线程池方式： 开一个线程池，每当爬虫发现一个新链接，就将链接放入任务队列中，线程池中的线程从任务队列获取一个链接，之后建立socket，完成抓取页面、解析、将新连接放入工作队列的步骤。\n",
    "* 回调方式：程序会有一个主循环叫做事件循环，在事件循环中会不断获得事件，通过在事件上注册解除回调函数来达到多任务并发执行的效果。缺点是一旦需要的回调操作变多，代码就会非常散，变得难以维护。\n",
    "* 协程方式：同样通过事件循环执行程序，利用了Python 的生成器特性，生成器函数能够中途停止并在之后恢复，那么原本不得不分开写的回调函数就能够写在一个生成器函数中了，这也就实现了协程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 多进程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 多线程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 异步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 异步的介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是异步\n",
    "（1）什么是同步IO和异步IO，它们之间有什么区别？\n",
    "\n",
    "答：举个现实例子，假设你需要打开4个不同的网站，但每个网站都比较卡。IO过程就相当于你打开网站的过程，CPU就是你的点击动作。你的点击动作很快，但是网站打开很慢。同步IO是指你每点击一个网址，都等待该网站彻底显示，才会去点击下一个网址。异步IO是指你点击完一个网址，不等对方服务器返回结果，立马新开浏览器窗口去打开另外一个网址，以此类推，最后同时等待4个网站彻底打开。很明显异步IO的效率更高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么使用异步\n",
    "传统计算机科学往往将大量精力放在如何追求更有效率的算法上。但如今大部分涉及网络的程序，它们的时间开销主要并不是在计算上，而是在维持多个Socket连接上。亦或是它们的事件循环处理的不够高效导致了更多的时间开销。对于这些程序来说，它们面临的挑战是如何更高效地等待大量的网络事件并进行调度。目前流行的解决方式就是使用异步I/O。\n",
    "\n",
    "本课程将探讨几种实现爬虫的方法，从传统的线程池到使用协程，每节课实现一个小爬虫。另外学习协程的时候，我们会从原理入手，以ayncio协程库为原型，实现一个简单的异步编程模型。\n",
    "\n",
    "本课程实现的爬虫为爬一个整站的爬虫，不会爬到站点外面去，且功能较简单，主要目的在于学习原理，提供实现并发与异步的思路，并不适合直接改写作为日常工具使用。\n",
    "\n",
    "本次实验我们将使用线程池实现一个爬虫。   \n",
    "\n",
    "资料来源：实验楼[Python实现基于协程的异步爬虫](https://www.shiyanlou.com/courses/574/labs/1905/document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 单线程上的异步I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "有了非阻塞I/O这个特性，我们就能够实现单线程上多个sockets的处理了，学过C语言网络编程的同学应该都认识select这个函数吧？不认识也不要紧，select函数如果你不设置它的超时时间它就是默认一直阻塞的，只有当有I/O事件发生时它才会被激活，然后告诉你哪个socket上发生了什么事件（读|写|异常），在Python中也有select，还有跟select功能相同但是更高效的poll，它们都是底层C函数的Python实现。\n",
    "\n",
    "不过这里我们不使用select，而是用更简单好用的DefaultSelector，是Python 3.4后才出现的一个模块里的类，你只需要在非阻塞socket和事件上绑定回调函数就可以了。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 协程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### 什么是协程？\n",
    "\n",
    "协程其实是比起一般的子例程而言更宽泛的存在，子例程是协程的一种特例。\n",
    "\n",
    "子例程的起始处是惟一的入口点，一旦退出即完成了子例程的执行，子例程的一个实例只会返回一次。\n",
    "\n",
    "协程可以通过<font color=#A52A2A size=4 >yield</font>来调用其它协程。通过<font color=#A52A2A size=4 >yield</font>方式转移执行权的协程之间不是调用者与被调用者的关系，而是彼此对称、平等的。\n",
    "\n",
    "协程的起始处是第一个入口点，在协程里，返回点之后是接下来的入口点。子例程的生命期遵循后进先出（最后一个被调用的子例程最先返回）；相反，协程的生命期完全由他们的使用的需要决定。\n",
    "\n",
    "还记得我们什么时候会用到<font color=#A52A2A size=4 >yield</font>吗，就是在生成器(generator)里，在迭代的时候每次执行next(generator)生成器都会执行到下一次<font color=#A52A2A size=4 >yield</font>的位置并返回，可以说生成器就是例程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#定义生成器函数\n",
    "def fib():\n",
    "    a, b = 0, 1\n",
    "    while(True):\n",
    "            yield a\n",
    "            a, b = b, a + b\n",
    "#获得生成器\n",
    "fib = fib()\n",
    "\n",
    "print(next(fib))     # >> 0\n",
    "print(next(fib))     # >> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<p>协程可以通过<code bgcolor=orange>yield</code>来调用其它协程。通过<code>yield</code>方式转移执行权的协程之间不是调用者与被调用者的关系，而是彼此对称、平等的。</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么用协程\n",
    "Python中解决IO密集型任务（打开多个网站）的方式有很多种，比如多进程、多线程。但理论上一台电脑中的线程数、进程数是有限的，而且进程、线程之间的切换也比较浪费时间。所以就出现了“协程”的概念。协程允许一个执行过程A中断，然后转到执行过程B，在适当的时候再一次转回来，有点类似于多线程。但协程有以下2个优势：\n",
    "\n",
    ">* 协程的数量理论上可以是无限个，而且没有线程之间的切换动作，执行效率比线程高。\n",
    "* 协程不需要“锁”机制，即不需要lock和release过程，因为所有的协程都在一个线程中。\n",
    "* 相对于线程，协程更容易调试debug，因为所有的代码是顺序执行的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### 协程的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### gevent协程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Python通过yield提供了对协程的基本支持，但是不完全。而第三方的gevent为Python提供了比较完善的协程支持。\n",
    "\n",
    "gevent是第三方库，通过greenlet实现协程，其基本思想是：\n",
    "\n",
    "当一个greenlet遇到IO操作时，比如访问网络，就自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换回来继续执行。由于IO操作非常耗时，经常使程序处于等待状态，有了gevent为我们自动切换协程，就保证总有greenlet在运行，而不是等待IO。\n",
    "\n",
    "由于切换是在IO操作时自动完成，所以gevent需要修改Python自带的一些标准库，这一过程在启动时通过monkey patch完成：\n",
    "gevent修改的过程很慢，因为时间问题就打断了。\n",
    "gevent的教程是在廖雪峰python2的协程里出现的，而在python3中就没有出现了，因此我怀疑gevent不兼容python3，这部分就先这样了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## 网页摘抄代码\n",
    "import requests  \n",
    "from multiprocessing import Process  \n",
    "import gevent  \n",
    "from gevent import monkey; monkey.patch_all()  \n",
    "  \n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')  \n",
    "def fetch(url):  \n",
    "    try:  \n",
    "        s = requests.Session()  \n",
    "        r = s.get(url,timeout=1)#在这里抓取页面  \n",
    "    except Exception as e:  \n",
    "        print(e)   \n",
    "    return ''  \n",
    "   \n",
    "def process_start(url_list):  \n",
    "    tasks = []  \n",
    "    for url in url_list:  \n",
    "        tasks.append(gevent.spawn(fetch,url))  \n",
    "    gevent.joinall(tasks)#使用协程来执行  \n",
    "  \n",
    "def task_start(filepath,flag = 100000):#每10W条url启动一个进程  \n",
    "    with open(filepath,'r') as reader:#从给定的文件中读取url  \n",
    "        url = reader.readline().strip()  \n",
    "        url_list = []#这个list用于存放协程任务  \n",
    "        i = 0 #计数器，记录添加了多少个url到协程队列  \n",
    "        while url!='':  \n",
    "            i += 1  \n",
    "            url_list.append(url)#每次读取出url，将url添加到队列  \n",
    "            if i == flag:#一定数量的url就启动一个进程并执行  \n",
    "                p = Process(target=process_start,args=(url_list,))  \n",
    "                p.start()  \n",
    "                url_list = [] #重置url队列  \n",
    "                i = 0 #重置计数器  \n",
    "            url = reader.readline().strip()  \n",
    "        if url_list not []:#若退出循环后任务队列里还有url剩余  \n",
    "            p = Process(target=process_start,args=(url_list,))#把剩余的url全都放到最后这个进程来执行  \n",
    "            p.start()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "url_list=[]\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        url_list.append('https://xxxbunker.com/html5player.php?videoid=8'+str(i)+str(j)+'1681&autoplay=true')\n",
    "\n",
    "start_time = time.time()\n",
    "process_start(url_list)\n",
    "print(time.time()-start_time)\n",
    "start_time = time.time()\n",
    "for i in url_list:\n",
    "    fetch(i)\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (LoopExit('This operation would block forever', <Hub at 0x92a53ff9c8 select pending=0 ref=0>)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "monkey.patch_all() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "测试模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\r\n",
      "<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=ç¾åº¦ä¸ä¸ class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>æ°é»</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>å°å¾</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>è§é¢</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>è´´å§</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>ç»å½</a> </noscript> <script>document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">ç»å½</a>');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">æ´å¤äº§å</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>å",
      "³äºç¾åº¦</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>ä½¿ç¨ç¾åº¦åå¿",
      "è¯»</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>æè§åé¦</a>&nbsp;äº¬ICPè¯030173å·&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url_list=['http://www.baidu.com']\n",
    "process_start(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def process_start(url_list):  \n",
    "    tasks = []  \n",
    "    for url in url_list:  \n",
    "        tasks.append(gevent.spawn(fetch,url))  \n",
    "    gevent.joinall(tasks)#使用协程来执行 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def fetch(url):  \n",
    "    try:  \n",
    "        s = requests.Session()  \n",
    "        r = s.get(url,timeout=1)#在这里抓取页面\n",
    "        #print(r.text)\n",
    "    except Exception as e:  \n",
    "        print(e)   \n",
    "    return '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import requests  \n",
    "from multiprocessing import Process  \n",
    "import gevent  \n",
    "from gevent import monkey\n",
    "#monkey.patch_all()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import requests  \n",
    "from multiprocessing import Process  \n",
    "import gevent  \n",
    "from gevent import monkey; monkey.patch_all()  \n",
    "  \n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')  \n",
    " \n",
    "\n",
    "def process_start(url_list):  \n",
    "    tasks = []  \n",
    "    for url in url_list:  \n",
    "        tasks.append(gevent.spawn(fetch,url))  \n",
    "    gevent.joinall(tasks)#使用协程来执行  \n",
    "  \n",
    "def task_start(filepath,flag = 100000):#每10W条url启动一个进程  \n",
    "    with open(filepath,'r') as reader:#从给定的文件中读取url  \n",
    "        url = reader.readline().strip()  \n",
    "        url_list = []#这个list用于存放协程任务  \n",
    "        i = 0 #计数器，记录添加了多少个url到协程队列  \n",
    "        while url!='':  \n",
    "            i += 1  \n",
    "            url_list.append(url)#每次读取出url，将url添加到队列  \n",
    "            if i == flag:#一定数量的url就启动一个进程并执行  \n",
    "                p = Process(target=process_start,args=(url_list,))  \n",
    "                p.start()  \n",
    "                url_list = [] #重置url队列  \n",
    "                i = 0 #重置计数器  \n",
    "            url = reader.readline().strip()  \n",
    "        if not url_list ==[]:#若退出循环后任务队列里还有url剩余  \n",
    "            p = Process(target=process_start,args=(url_list,))#把剩余的url全都放到最后这个进程来执行  \n",
    "            p.start()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "asyncio是Python 3.4版本引入的标准库，直接内置了对异步IO的支持。\n",
    "\n",
    "asyncio的编程模型就是一个消息循环。我们从asyncio模块中直接获取一个EventLoop的引用，然后把需要执行的协程扔到EventLoop中执行，就实现了异步IO。\n",
    "\n",
    "用asyncio实现Hello world代码如下：（运行前需要restart kernel）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "Hello again!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "@asyncio.coroutine\n",
    "def hello():\n",
    "    print(\"Hello world!\")\n",
    "    # 异步调用asyncio.sleep(1):\n",
    "    r = yield from asyncio.sleep(1)\n",
    "    print(\"Hello again!\")\n",
    "\n",
    "# 获取EventLoop:\n",
    "loop = asyncio.get_event_loop()\n",
    "# 执行coroutine\n",
    "loop.run_until_complete(hello())\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "@asyncio.coroutine把一个generator标记为coroutine类型，然后，我们就把这个coroutine扔到EventLoop中执行。\n",
    "\n",
    "hello()会首先打印出Hello world!，然后，yield from语法可以让我们方便地调用另一个generator。由于asyncio.sleep()也是一个coroutine，所以线程不会等待asyncio.sleep()，而是直接中断并执行下一个消息循环。当asyncio.sleep()返回时，线程就可以从yield from拿到返回值（此处是None），然后接着执行下一行语句。\n",
    "\n",
    "把asyncio.sleep(1)看成是一个耗时1秒的IO操作，在此期间，主线程并未等待，而是去执行EventLoop中其他可以执行的coroutine了，因此可以实现并发执行。\n",
    "\n",
    "我们用Task封装两个coroutine试试：（运行前需要restart kernel）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! (<_MainThread(MainThread, started 9416)>)\n",
      "Hello world! (<_MainThread(MainThread, started 9416)>)\n",
      "Hello again! (<_MainThread(MainThread, started 9416)>)\n",
      "Hello again! (<_MainThread(MainThread, started 9416)>)\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import asyncio\n",
    "\n",
    "@asyncio.coroutine\n",
    "def hello():\n",
    "    print('Hello world! (%s)' % threading.currentThread())\n",
    "    yield from asyncio.sleep(1)\n",
    "    print('Hello again! (%s)' % threading.currentThread())\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "tasks = [hello(), hello()]\n",
    "loop.run_until_complete(asyncio.wait(tasks))\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由打印的当前线程名称可以看出，两个coroutine是由同一个线程并发执行的。\n",
    "\n",
    "如果把asyncio.sleep()换成真正的IO操作，则多个coroutine就可以由一个线程并发执行。\n",
    "\n",
    "我们用asyncio的异步网络连接来获取sina、sohu和163的网站首页："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget www.sohu.com...\n",
      "wget www.sina.com.cn...\n",
      "wget www.163.com...\n",
      "www.sohu.com header > HTTP/1.1 200 OK\n",
      "www.sohu.com header > Content-Type: text/html;charset=UTF-8\n",
      "www.sohu.com header > Connection: close\n",
      "www.sohu.com header > Server: nginx\n",
      "www.sohu.com header > Date: Sun, 19 Nov 2017 05:02:58 GMT\n",
      "www.sohu.com header > Cache-Control: max-age=60\n",
      "www.sohu.com header > X-From-Sohu: X-SRC-Cached\n",
      "www.sohu.com header > Content-Encoding: gzip\n",
      "www.sohu.com header > FSS-Cache: HIT from 8605877.15552703.9605663\n",
      "www.sohu.com header > FSS-Proxy: Powered by 3100769.4542571.4100471\n",
      "www.163.com header > HTTP/1.0 302 Moved Temporarily\n",
      "www.163.com header > Server: Cdn Cache Server V2.0\n",
      "www.163.com header > Date: Sun, 19 Nov 2017 05:03:58 GMT\n",
      "www.163.com header > Content-Length: 0\n",
      "www.163.com header > Location: http://www.163.com/special/0077jt/error_isp.html\n",
      "www.163.com header > Connection: close\n",
      "www.sina.com.cn header > HTTP/1.1 200 OK\n",
      "www.sina.com.cn header > Server: nginx\n",
      "www.sina.com.cn header > Date: Sun, 19 Nov 2017 05:03:58 GMT\n",
      "www.sina.com.cn header > Content-Type: text/html\n",
      "www.sina.com.cn header > Content-Length: 602435\n",
      "www.sina.com.cn header > Connection: close\n",
      "www.sina.com.cn header > Last-Modified: Sun, 19 Nov 2017 05:03:06 GMT\n",
      "www.sina.com.cn header > Vary: Accept-Encoding\n",
      "www.sina.com.cn header > Expires: Sun, 19 Nov 2017 05:04:54 GMT\n",
      "www.sina.com.cn header > Cache-Control: max-age=60\n",
      "www.sina.com.cn header > X-Powered-By: shci_v1.03\n",
      "www.sina.com.cn header > Age: 5\n",
      "www.sina.com.cn header > Via: http/1.1 cnc.beixian.ha2ts4.205 (ApacheTrafficServer/6.2.1 [cMsSfW]), http/1.1 gwbn.beijing.ha2ts4.23 (ApacheTrafficServer/6.2.1 [cHs f ])\n",
      "www.sina.com.cn header > X-Via-Edge: 15110678387195bf2dc310904eedb400214fe\n",
      "www.sina.com.cn header > X-Cache: HIT.23\n",
      "www.sina.com.cn header > X-Via-CDN: f=edge,s=gwbn.beijing.ha2ts4.24.nb.sinaedge.com,c=49.220.242.91;f=Edge,s=gwbn.beijing.ha2ts4.23,c=49.220.242.91\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "@asyncio.coroutine\n",
    "def wget(host):\n",
    "    print('wget %s...' % host)\n",
    "    connect = asyncio.open_connection(host, 80)\n",
    "    reader, writer = yield from connect\n",
    "    header = 'GET / HTTP/1.0\\r\\nHost: %s\\r\\n\\r\\n' % host\n",
    "    writer.write(header.encode('utf-8'))\n",
    "    yield from writer.drain()\n",
    "    while True:\n",
    "        line = yield from reader.readline()\n",
    "        if line == b'\\r\\n':\n",
    "            break\n",
    "        print('%s header > %s' % (host, line.decode('utf-8').rstrip()))\n",
    "    # Ignore the body, close the socket\n",
    "    writer.close()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "tasks = [wget(host) for host in ['www.sina.com.cn', 'www.sohu.com', 'www.163.com']]\n",
    "loop.run_until_complete(asyncio.wait(tasks))\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见3个连接由一个线程通过coroutine并发完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### async/await"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：asyncio是3.4增加的特性，async/await是3.5增加的\n",
    "\n",
    "用asyncio提供的@asyncio.coroutine可以把一个generator标记为coroutine类型，然后在coroutine内部用yield from调用另一个coroutine实现异步操作。\n",
    "\n",
    "为了简化并更好地标识异步IO，从Python 3.5开始引入了新的语法async和await，可以让coroutine的代码更简洁易读。\n",
    "\n",
    "请注意，async和await是针对coroutine的新语法，要使用新的语法，只需要做两步简单的替换：\n",
    "\n",
    "把@asyncio.coroutine替换为async；\n",
    "把yield from替换为await。\n",
    "让我们对比一下上一节的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@asyncio.coroutine\n",
    "def hello():\n",
    "    print(\"Hello world!\")\n",
    "    r = yield from asyncio.sleep(1)\n",
    "    print(\"Hello again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用新语法重新编写如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def hello():\n",
    "    print(\"Hello world!\")\n",
    "    r = await asyncio.sleep(1)\n",
    "    print(\"Hello again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剩下的代码保持不变。\n",
    "\n",
    "小结\n",
    "\n",
    "Python从3.5版本开始为asyncio提供了async和await的新语法；\n",
    "\n",
    "注意新语法只能用在Python 3.5以及后续版本，如果使用3.4版本，则仍需使用上一节的方案。\n",
    "\n",
    "练习\n",
    "\n",
    "将上一节的异步获取sina、sohu和163的网站首页源码用新语法重写并运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asyncio可以实现单线程并发IO操作。如果仅用在客户端，发挥的威力不大。如果把asyncio用在服务器端，例如Web服务器，由于HTTP连接就是IO操作，因此可以用单线程+coroutine实现多用户的高并发支持。\n",
    "\n",
    "asyncio实现了TCP、UDP、SSL等协议，aiohttp则是基于asyncio实现的HTTP框架。\n",
    "\n",
    "我们先安装aiohttp："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后编写一个HTTP服务器，分别处理以下URL：\n",
    "\n",
    "*  \\- 首页返回b'<\\h1\\>Index\\</\\h1>'；\n",
    "\n",
    "* /hello/{name} - 根据URL参数返回文本hello, %s!。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from aiohttp import web\n",
    "\n",
    "async def index(request):\n",
    "    await asyncio.sleep(0.5)\n",
    "    return web.Response(body=b'<h1>Index</h1>')\n",
    "\n",
    "async def hello(request):\n",
    "    await asyncio.sleep(0.5)\n",
    "    text = '<h1>hello, %s!</h1>' % request.match_info['name']\n",
    "    return web.Response(body=text.encode('utf-8'))\n",
    "\n",
    "async def init(loop):\n",
    "    app = web.Application(loop=loop)\n",
    "    app.router.add_route('GET', '/', index)\n",
    "    app.router.add_route('GET', '/hello/{name}', hello)\n",
    "    srv = await loop.create_server(app.make_handler(), '127.0.0.1', 8000)\n",
    "    print('Server started at http://127.0.0.1:8000...')\n",
    "    return srv\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(init(loop))\n",
    "loop.run_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意aiohttp的初始化函数init()也是一个coroutine，loop.create_server()则利用asyncio创建TCP服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个来自知乎[Python进阶：理解Python中的异步IO和协程(Coroutine)，并应用在爬虫中](https://zhuanlan.zhihu.com/p/24118476)的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def get(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as resp:\n",
    "            print(url, resp.status)\n",
    "            print(url, await resp.text())\n",
    "\n",
    "loop = asyncio.get_event_loop()     # 得到一个事件循环模型\n",
    "tasks = [                           # 初始化任务列表\n",
    "    get(\"http://zhushou.360.cn/detail/index/soft_id/3283370\"),\n",
    "    get(\"http://zhushou.360.cn/detail/index/soft_id/3264775\"),\n",
    "    get(\"http://zhushou.360.cn/detail/index/soft_id/705490\")\n",
    "]\n",
    "loop.run_until_complete(asyncio.wait(tasks))    # 执行任务\n",
    "loop.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异步爬虫示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 豌豆荚设计奖多进程，异步IO爬取速度对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来源：简书[豌豆荚设计奖多进程，异步IO爬取速度对比](http://www.jianshu.com/p/989005adcccd)\n",
    "\n",
    "1. 豌豆荚的设计奖首页是<http://www.wandoujia.com/award> 点击下一页之后就会发现网页地址变成了http://www.wandoujia.com/award?page=x x就是当前的页数。\n",
    "![豌豆荚设计奖首页](img_summary/豌豆荚设计奖首页.png)\n",
    "2. 然后来看看本次抓取的信息分布，我抓取的是每个设计奖的背景图片，APP名称，图标，获奖说明。进入浏览器开发者模式后即可查找信息位置。（使用Ctrl+Shift+C选择目标快速到达代码位置，同时这个夸克浏览器也挺不错的，简洁流畅推荐大家安装试试。）\n",
    "![豌豆荚设计奖item解析](img_summary/豌豆荚设计奖item解析.png)\n",
    "3. 信息位置都找到了就可以使用BeautifulSoup来解析网页选择到这些数据，然后保存到mongodb。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 共用部分\n",
    "clients = pymongo.MongoClient('localhost')\n",
    "db = clients[\"wandoujia\"]\n",
    "col = db[\"info\"]\n",
    "\n",
    "urls = ['http://www.wandoujia.com/award?page={}'.format(num) for num in range(1, 46)]\n",
    "UA_LIST = [\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\"\n",
    "]\n",
    "headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "    'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Host': 'www.wandoujia.com',\n",
    "    'User-Agent': random.choice(UA_LIST)\n",
    "}\n",
    "\n",
    "proxies = {\n",
    "    'http': 'http://123.206.6.17:3128',\n",
    "    'https': 'http://123.206.6.17:3128'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用这个方法需要对每个函数前面加async，表示成一个异步函数，调用asyncio.get_event_loop创建线程，run_until_complete方法负责安排执行 tasks中的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def method_3():\n",
    "    async def get_url(url):\n",
    "        async with aiohttp.ClientSession() as session:  # await关键字将暂停协程函数的执行，等待异步IO返回结果。\n",
    "            async with session.get(url) as html:\n",
    "                response = await html.text(encoding=\"utf-8\")  # await关键字将暂停协程函数的执行，等待异步IO返回结果。\n",
    "                return response\n",
    "\n",
    "    async def parser(url):\n",
    "        html = await get_url(url)\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        title = soup.find_all(class_='title')\n",
    "        app_title = soup.find_all(class_='app-title')\n",
    "        item_cover = soup.find_all(class_='item-cover')\n",
    "        icon_cover = soup.select('div.list-wrap > ul > li > div.icon > img')\n",
    "        for title_i, app_title_i, item_cover_i, icon_cover_i in zip(title, app_title, item_cover, icon_cover):\n",
    "            content = {\n",
    "                'title': title_i.get_text(),\n",
    "                'app_title': app_title_i.get_text(),\n",
    "                'item_cover': item_cover_i['data-original'],\n",
    "                'icon_cover': icon_cover_i['data-original']\n",
    "            }\n",
    "            col.insert(content)\n",
    "            print('成功插入一组数据' + str(content))\n",
    "\n",
    "    start = time.time()\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [parser(url) for url in urls]\n",
    "    loop.run_until_complete(asyncio.gather(*tasks))\n",
    "    print(time.time() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 框架的使用\n",
    "## scrapy的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### scrapy介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Scrapy简明教程(四)——爬取CSDN博客专家所有博文并存入MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "首先，我们来看一下CSDN博客专家的链接: http://blog.csdn.net/experts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 分布式爬虫"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "511px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "503px",
    "left": "0px",
    "right": "1047.67px",
    "top": "130px",
    "width": "221px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
